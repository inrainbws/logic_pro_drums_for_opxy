#!/usr/bin/env python3
"""
Splice bounced Logic Pro drum audio and export as OP-XY drum kit.

This script:
1. Reads a bounced audio file containing sequential drum hits
2. Splits it into individual samples using timing info
3. Trims silence from each sample
4. Exports as OP-XY compatible drum kit (ZIP with patch.json + WAVs)

Usage:
    python splice_and_export.py audio.wav --timing drum_trigger.timing.txt --name "My Kit"

Requirements:
    pip install soundfile numpy scipy
"""

import argparse
import json
import zipfile
from pathlib import Path
from dataclasses import dataclass
from typing import Optional
import struct
import io

try:
    import numpy as np
    import soundfile as sf
except ImportError:
    print("Error: Required libraries missing. Install with:")
    print("  pip install soundfile numpy")
    exit(1)


@dataclass
class DrumSample:
    """Represents a single drum sample."""
    index: int
    midi_note: int
    name: str
    start_time: float
    duration: float
    audio: Optional[np.ndarray] = None
    sample_rate: int = 44100


# Default mapping file path (same directory as script)
DEFAULT_MAPPING_FILE = Path(__file__).parent / "logic_drum_mapping_standard.json"


def get_duration_for_sound(name: str) -> float:
    """Determine appropriate duration based on sound type."""
    name_lower = name.lower()

    # Cymbals need long decay
    if any(word in name_lower for word in ['crash', 'china', 'splash']):
        return 5.0
    if any(word in name_lower for word in ['ride', 'cymbal']):
        return 4.0

    # Hi-hats
    if 'open' in name_lower and 'hat' in name_lower:
        return 3.0
    if 'hat' in name_lower:
        return 2.0

    # Toms
    if 'tom' in name_lower or 'floor' in name_lower:
        return 2.5

    # Short percussion
    if any(word in name_lower for word in ['kick', 'snare', 'clap', 'stick', 'cowbell', 'claves']):
        return 2.0

    # Sustained percussion
    if any(word in name_lower for word in ['tambourine', 'shaker', 'vibraslap']):
        return 2.5

    # Default
    return 2.0


def load_drum_mapping_json(mapping_file: Path) -> list[tuple[int, str, float]]:
    """
    Load drum mapping from JSON file and add durations.

    Supports two formats:
    1. Array format (preferred for OP-XY compatibility):
       [{"note": 36, "name": "Kick"}, ...]
       Order is preserved - index determines OP-XY slot.

    2. Object format (legacy):
       {"36": "Kick", ...}
       Sorted by MIDI note number.
    """
    with open(mapping_file, 'r') as f:
        mapping = json.load(f)

    drum_list = []

    if isinstance(mapping, list):
        # Array format - preserve order for OP-XY slot compatibility
        for item in mapping:
            note = item["note"]
            name = item["name"]
            duration = get_duration_for_sound(name)
            drum_list.append((note, name, duration))
    else:
        # Object format - sort by MIDI note
        for note_str, name in mapping.items():
            note = int(note_str)
            duration = get_duration_for_sound(name)
            drum_list.append((note, name, duration))
        drum_list.sort(key=lambda x: x[0])

    return drum_list

# OP-XY drum kit MIDI mapping (F#3 to F5, notes 53-76)
OPXY_SLOT_NOTES = list(range(53, 77))  # 24 slots


def load_timing_info(timing_file: str) -> list[DrumSample]:
    """Load timing information from file generated by generate_drum_midi.py."""
    samples = []

    with open(timing_file, 'r') as f:
        for line in f:
            line = line.strip()
            if line.startswith('#') or not line:
                continue
            parts = line.split(',')
            if len(parts) >= 5:
                idx = int(parts[0])
                midi_note = int(parts[1])
                name = parts[2]
                start_time = float(parts[3])
                duration = float(parts[4])
                samples.append(DrumSample(
                    index=idx,
                    midi_note=midi_note,
                    name=name,
                    start_time=start_time,
                    duration=duration
                ))

    return samples


def generate_default_timing(lead_in: float = 0.5, mapping_file: Path = DEFAULT_MAPPING_FILE) -> list[DrumSample]:
    """Generate default timing based on JSON drum mapping."""
    drum_mapping = load_drum_mapping_json(mapping_file)

    samples = []
    current_time = lead_in

    for idx, (midi_note, name, duration) in enumerate(drum_mapping):
        samples.append(DrumSample(
            index=idx,
            midi_note=midi_note,
            name=name,
            start_time=current_time,
            duration=duration
        ))
        current_time += duration

    return samples


def detect_silence_end(
    audio: np.ndarray,
    sample_rate: int,
    threshold_db: float = -60,
    min_silence_ms: float = 100
) -> int:
    """
    Find the end of audio content (before sustained silence).

    Args:
        audio: Audio data (mono or stereo)
        sample_rate: Sample rate
        threshold_db: Silence threshold in dB
        min_silence_ms: Minimum silence duration to consider as end

    Returns:
        Sample index where audio effectively ends
    """
    # Convert to mono if stereo
    if len(audio.shape) > 1:
        mono = np.mean(audio, axis=1)
    else:
        mono = audio

    # Convert threshold to linear
    threshold_linear = 10 ** (threshold_db / 20)

    # Calculate RMS in small windows
    window_size = int(sample_rate * 0.01)  # 10ms windows
    min_silence_samples = int(sample_rate * min_silence_ms / 1000)

    # Find absolute values
    abs_audio = np.abs(mono)

    # Scan from end to find last loud sample
    consecutive_silence = 0
    last_loud_sample = len(mono) - 1

    for i in range(len(mono) - 1, -1, -window_size):
        window_start = max(0, i - window_size)
        window = abs_audio[window_start:i]
        if len(window) == 0:
            continue

        rms = np.sqrt(np.mean(window ** 2))

        if rms > threshold_linear:
            # Found audio content
            last_loud_sample = i
            break

    # Add a small tail (50ms) for natural decay
    tail_samples = int(sample_rate * 0.05)
    end_sample = min(last_loud_sample + tail_samples, len(mono))

    return end_sample


def trim_start_silence(
    audio: np.ndarray,
    sample_rate: int,
    threshold_db: float = -50,
    lookback_ms: float = 5
) -> int:
    """
    Find the start of audio content (skip leading silence).

    Args:
        audio: Audio data
        sample_rate: Sample rate
        threshold_db: Silence threshold in dB
        lookback_ms: Milliseconds to look back for attack

    Returns:
        Sample index where audio starts
    """
    if len(audio.shape) > 1:
        mono = np.mean(audio, axis=1)
    else:
        mono = audio

    threshold_linear = 10 ** (threshold_db / 20)
    abs_audio = np.abs(mono)

    # Find first sample above threshold
    above_threshold = np.where(abs_audio > threshold_linear)[0]

    if len(above_threshold) == 0:
        return 0

    first_loud = above_threshold[0]

    # Look back slightly to catch attack transient
    lookback_samples = int(sample_rate * lookback_ms / 1000)
    start_sample = max(0, first_loud - lookback_samples)

    return start_sample


def sanitize_filename(name: str) -> str:
    """Sanitize name for use as filename."""
    # Replace spaces with underscores, remove special chars
    clean = name.replace(' ', '_').replace('-', '_')
    clean = ''.join(c for c in clean if c.isalnum() or c == '_')
    return clean.lower()


def create_patch_json(
    kit_name: str,
    samples: list[DrumSample],
    sample_rate: int = 44100
) -> dict:
    """Create OP-XY patch.json structure."""

    patch = {
        "name": kit_name,
        "platform": "OP-XY",
        "type": "drum",
        "version": 4,
        "octave": 0,
        "engine": {
            "bendrange": 8191,
            "highpass": 0,
            "modulation": {
                "aftertouch": {"amount": 16383, "target": 0},
                "modwheel": {"amount": 16383, "target": 0},
                "pitchbend": {"amount": 16383, "target": 0},
                "velocity": {"amount": 16383, "target": 0}
            },
            "params": [16384, 16384, 16384, 16384, 16384, 16384, 16384, 16384],
            "playmode": "poly",
            "portamento.amount": 0,
            "portamento.type": 32767,
            "transpose": 0,
            "tuning.root": 0,
            "tuning.scale": 0,
            "velocity.sensitivity": 29490,
            "volume": 18348,
            "width": 0
        },
        "envelope": {
            "amp": {"attack": 0, "decay": 0, "release": 1000, "sustain": 32767},
            "filter": {"attack": 0, "decay": 3276, "release": 23757, "sustain": 983}
        },
        "fx": {
            "active": False,
            "params": [22014, 0, 30285, 11880, 0, 32767, 0, 0],
            "type": "ladder"
        },
        "lfo": {
            "active": False,
            "params": [20309, 5679, 19114, 15807, 0, 0, 0, 12287],
            "type": "random"
        },
        "regions": []
    }

    # Create regions for each sample
    for i, sample in enumerate(samples):
        if sample.audio is None:
            continue

        framecount = len(sample.audio) if len(sample.audio.shape) == 1 else sample.audio.shape[0]
        filename = f"{i+1:02d}_{sanitize_filename(sample.name)}.wav"
        opxy_note = OPXY_SLOT_NOTES[i] if i < len(OPXY_SLOT_NOTES) else 53 + i

        region = {
            "fade.in": 0,
            "fade.out": 0,
            "framecount": framecount,
            "hikey": opxy_note,
            "lokey": opxy_note,
            "pan": 0,
            "pitch.keycenter": 60,
            "playmode": "oneshot",
            "reverse": False,
            "sample": filename,
            "transpose": 0,
            "tune": 0,
            "gain": 0,
            "sample.start": 0,
            "sample.end": framecount
        }
        patch["regions"].append(region)

    return patch


def process_audio(
    audio_file: str,
    samples: list[DrumSample],
    silence_threshold_db: float = -60,
    min_length_ms: float = 100,
    max_length_ms: float = 10000
) -> list[DrumSample]:
    """
    Extract individual samples from concatenated audio.

    Args:
        audio_file: Path to bounced audio file
        samples: List of DrumSample with timing info
        silence_threshold_db: Threshold for silence detection
        min_length_ms: Minimum sample length in ms
        max_length_ms: Maximum sample length in ms

    Returns:
        List of DrumSample with audio data filled in
    """
    print(f"Loading audio: {audio_file}")
    audio, sample_rate = sf.read(audio_file)

    print(f"  Sample rate: {sample_rate} Hz")
    print(f"  Duration: {len(audio) / sample_rate:.2f} seconds")
    print(f"  Channels: {audio.shape[1] if len(audio.shape) > 1 else 1}")

    min_samples = int(sample_rate * min_length_ms / 1000)
    max_samples = int(sample_rate * max_length_ms / 1000)

    print(f"\nExtracting {len(samples)} samples...")
    print("-" * 60)

    # Safety margin before next sample's attack (in seconds)
    safety_margin_sec = 0.02  # 20ms

    for i, sample in enumerate(samples):
        # Calculate sample boundaries
        start_sample = int(sample.start_time * sample_rate)
        max_end_sample = int((sample.start_time + sample.duration) * sample_rate)

        # IMPORTANT: Cap at next sample's start time to avoid capturing its attack
        if i < len(samples) - 1:
            next_start_sample = int(samples[i + 1].start_time * sample_rate)
            # Leave safety margin before next sample
            hard_boundary = next_start_sample - int(safety_margin_sec * sample_rate)
            max_end_sample = min(max_end_sample, hard_boundary)

        # Ensure we don't go past audio end
        max_end_sample = min(max_end_sample, len(audio))
        start_sample = min(start_sample, len(audio) - 1)

        if start_sample >= max_end_sample:
            print(f"  [{sample.index+1:2d}] {sample.name:20s} - SKIPPED (beyond audio)")
            continue

        # Extract segment (hard-bounded to not overlap next sample)
        segment = audio[start_sample:max_end_sample]

        # Trim leading silence
        trim_start = trim_start_silence(segment, sample_rate, threshold_db=-45)

        # Detect end of audio content within the bounded segment
        trim_end = detect_silence_end(
            segment[trim_start:],
            sample_rate,
            threshold_db=silence_threshold_db
        )

        # Apply constraints (already bounded by segment extraction)
        actual_length = trim_end
        actual_length = max(actual_length, min_samples)
        actual_length = min(actual_length, max_samples, len(segment) - trim_start)

        # Extract trimmed audio
        trimmed = segment[trim_start:trim_start + actual_length]

        # Normalize (peak normalize to -1dB)
        if len(trimmed.shape) > 1:
            peak = np.max(np.abs(trimmed))
        else:
            peak = np.max(np.abs(trimmed))

        if peak > 0:
            target_peak = 10 ** (-1 / 20)  # -1dB
            trimmed = trimmed * (target_peak / peak)

        sample.audio = trimmed
        sample.sample_rate = sample_rate

        duration_ms = len(trimmed) / sample_rate * 1000
        print(f"  [{sample.index+1:2d}] {sample.name:20s} - {duration_ms:6.0f}ms")

    return samples


def export_opxy_kit(
    samples: list[DrumSample],
    output_path: str,
    kit_name: str,
    sample_rate: int = 44100,
    bit_depth: int = 16
):
    """
    Export samples as OP-XY drum kit ZIP.

    Args:
        samples: List of processed DrumSample objects
        output_path: Output ZIP file path
        kit_name: Name of the drum kit
        sample_rate: Target sample rate (will resample if needed)
        bit_depth: Output bit depth (16 or 24)
    """
    print(f"\nExporting OP-XY kit: {output_path}")

    # Filter samples with audio
    valid_samples = [s for s in samples if s.audio is not None]

    if not valid_samples:
        print("Error: No valid samples to export!")
        return

    # Create patch.json
    patch = create_patch_json(kit_name, valid_samples, sample_rate)

    # Determine subtype for soundfile
    if bit_depth == 24:
        subtype = 'PCM_24'
    else:
        subtype = 'PCM_16'

    # Create ZIP file
    with zipfile.ZipFile(output_path, 'w', zipfile.ZIP_DEFLATED) as zf:
        # Add patch.json
        patch_json = json.dumps(patch, indent=2)
        zf.writestr('patch.json', patch_json)

        # Add audio files
        for i, sample in enumerate(valid_samples):
            filename = f"{i+1:02d}_{sanitize_filename(sample.name)}.wav"

            # Resample if necessary
            audio_data = sample.audio
            current_rate = sample.sample_rate

            if current_rate != sample_rate:
                # Simple resampling using scipy if available
                try:
                    from scipy import signal
                    num_samples = int(len(audio_data) * sample_rate / current_rate)
                    if len(audio_data.shape) > 1:
                        # Stereo
                        resampled = np.zeros((num_samples, audio_data.shape[1]))
                        for ch in range(audio_data.shape[1]):
                            resampled[:, ch] = signal.resample(audio_data[:, ch], num_samples)
                        audio_data = resampled
                    else:
                        audio_data = signal.resample(audio_data, num_samples)
                except ImportError:
                    print(f"  Warning: scipy not available, keeping original sample rate")

            # Write WAV to bytes buffer
            buffer = io.BytesIO()
            sf.write(buffer, audio_data, sample_rate, subtype=subtype, format='WAV')
            buffer.seek(0)

            zf.writestr(filename, buffer.read())

            print(f"  Added: {filename}")

    # Check file size
    file_size = Path(output_path).stat().st_size
    size_mb = file_size / (1024 * 1024)
    print(f"\nKit exported: {output_path}")
    print(f"  Size: {size_mb:.2f} MB {'(OK)' if size_mb < 8 else '(WARNING: exceeds 8MB limit)'}")
    print(f"  Samples: {len(valid_samples)}")


def main():
    parser = argparse.ArgumentParser(
        description="Splice Logic Pro drum bounce and export as OP-XY drum kit"
    )
    parser.add_argument(
        "audio",
        help="Input audio file (WAV or AIFF)"
    )
    parser.add_argument(
        "--timing",
        help="Timing file from generate_drum_midi.py (auto-detected if not specified)"
    )
    parser.add_argument(
        "--name",
        default="Logic Drum Kit",
        help="Drum kit name (default: 'Logic Drum Kit')"
    )
    parser.add_argument(
        "--output",
        help="Output ZIP file path (default: based on kit name)"
    )
    parser.add_argument(
        "--threshold",
        type=float,
        default=-80,
        help="Silence detection threshold in dB (default: -80)"
    )
    parser.add_argument(
        "--min-length",
        type=float,
        default=100,
        help="Minimum sample length in ms (default: 100)"
    )
    parser.add_argument(
        "--max-length",
        type=float,
        default=10000,
        help="Maximum sample length in ms (default: 10000)"
    )
    parser.add_argument(
        "--sample-rate",
        type=int,
        default=44100,
        choices=[11025, 22050, 44100],
        help="Output sample rate (default: 44100)"
    )
    parser.add_argument(
        "--bit-depth",
        type=int,
        default=16,
        choices=[16, 24],
        help="Output bit depth (default: 16)"
    )

    args = parser.parse_args()

    # Check input file exists
    audio_path = Path(args.audio)
    if not audio_path.exists():
        print(f"Error: Audio file not found: {args.audio}")
        exit(1)

    # Load or generate timing info
    if args.timing:
        timing_path = Path(args.timing)
        if not timing_path.exists():
            print(f"Error: Timing file not found: {args.timing}")
            exit(1)
        samples = load_timing_info(args.timing)
        print(f"Loaded timing info: {len(samples)} samples")
    else:
        # Try to auto-detect timing file
        auto_timing = audio_path.parent / "drum_trigger.timing.txt"
        if auto_timing.exists():
            samples = load_timing_info(str(auto_timing))
            print(f"Auto-detected timing file: {auto_timing}")
            print(f"Loaded timing info: {len(samples)} samples")
        else:
            print("No timing file found, using default timing")
            samples = generate_default_timing()

    # Process audio
    samples = process_audio(
        args.audio,
        samples,
        silence_threshold_db=args.threshold,
        min_length_ms=args.min_length,
        max_length_ms=args.max_length
    )

    # Determine output path
    if args.output:
        output_path = args.output
    else:
        safe_name = sanitize_filename(args.name)
        output_path = str(audio_path.parent / f"{safe_name}.preset.zip")

    # Export
    export_opxy_kit(
        samples,
        output_path,
        args.name,
        sample_rate=args.sample_rate,
        bit_depth=args.bit_depth
    )

    print(f"\nDone! Transfer {output_path} to your OP-XY.")


if __name__ == "__main__":
    main()
